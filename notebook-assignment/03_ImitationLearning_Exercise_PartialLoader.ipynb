{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRnY963jq1CW"
   },
   "source": [
    "Authors: Hofmarcher, Dinu, Radler\n",
    "\n",
    "Date: 23-03-2021\n",
    "\n",
    "---\n",
    "\n",
    "This file is part of the \"Deep Reinforcement Learning\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "Copyright statement:\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eryXuKadrmRK"
   },
   "source": [
    "## Enable GPU Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e5BB3G1hXL5"
   },
   "source": [
    "---\n",
    "Before you start exploring this notebook make sure that GPU support is enabled.\n",
    "To enable the GPU backend for your notebook, go to **Edit** â†’ **Notebook Settings** and set **Hardware accelerator** to **GPU**. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBUs4yMsgRSz"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D64rNsQCyL6Q"
   },
   "source": [
    "Install OpenAI Gym and dependencies to render the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqL6W_Gkgp9a"
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt install -y xvfb x11-utils python-opengl ffmpeg \n",
    "!pip install gym==0.17.3 pyvirtualdisplay \n",
    "!pip install box2d-py\n",
    "!pip install gym[box2d]\n",
    "!pip install onnx onnx2pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ECmcPAOnhR4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, ToPILImage\n",
    "import onnx\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "# Auxiliary Python imports\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep, time, strftime\n",
    "\n",
    "# Environment import and set logger level to display error only\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "\n",
    "# Plotting and notebook imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import HTML, clear_output\n",
    "from IPython import display\n",
    "\n",
    "# start virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "pydisplay = Display(visible=0, size=(640, 480))\n",
    "pydisplay.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De3CuyBLgXos"
   },
   "source": [
    "## Setup Google Drive mount to store your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vpd2i1MumjP0"
   },
   "outputs": [],
   "source": [
    "mount_google_drive = False\n",
    "if mount_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdVZakrRnX6V"
   },
   "source": [
    "# Download Expert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFywU9x-nWvK"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/26Hpzm3q2WgfRi8/download' -O expert.onnx # expert agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IuPNzGVgGR0"
   },
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy79qvPFfLRp"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/Qp7rjfEmcSsAek9/download' -O train.zip # split train data\n",
    "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/NcYgkfW8Ss6tNzw/download' -O val.zip # split val data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5chkQUwj4pT"
   },
   "source": [
    "# Auxiliary Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrjYL01ojsAD"
   },
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, logdir, params=None):\n",
    "        self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
    "        os.makedirs(self.basepath, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        if params is not None and os.path.exists(params):\n",
    "            shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n",
    "        self.log_dict = {}\n",
    "        self.dump_idx = {}\n",
    "\n",
    "    @property\n",
    "    def param_file(self):\n",
    "        return os.path.join(self.basepath, \"params.pkl\")\n",
    "\n",
    "    @property\n",
    "    def onnx_file(self):\n",
    "        return os.path.join(self.basepath, \"model.onnx\")\n",
    "\n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        return os.path.join(self.basepath, \"logs\")\n",
    "\n",
    "    def log(self, name, value):\n",
    "        if name not in self.log_dict:\n",
    "            self.log_dict[name] = []\n",
    "            self.dump_idx[name] = -1\n",
    "        self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n",
    "    \n",
    "    def get_values(self, name):\n",
    "        if name in self.log_dict:\n",
    "            return [x[2] for x in self.log_dict[name]]\n",
    "        return None\n",
    "    \n",
    "    def dump(self):\n",
    "        for name, rows in self.log_dict.items():\n",
    "            with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n",
    "                for i, row in enumerate(rows):\n",
    "                    if i > self.dump_idx[name]:\n",
    "                        f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n",
    "                        self.dump_idx[name] = i\n",
    "\n",
    "\n",
    "def plot_metrics(logger):\n",
    "    train_loss  = logger.get_values(\"training_loss\")\n",
    "    train_entropy  = logger.get_values(\"training_entropy\")\n",
    "    val_loss = logger.get_values(\"validation_loss\")\n",
    "    val_acc = logger.get_values(\"validation_accuracy\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax1 = fig.add_subplot(131, label=\"train\")\n",
    "    ax2 = fig.add_subplot(131, label=\"val\",frame_on=False)\n",
    "    ax4 = fig.add_subplot(132, label=\"entropy\")\n",
    "    ax3 = fig.add_subplot(133, label=\"acc\")\n",
    "\n",
    "    ax1.plot(train_loss, color=\"C0\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_xlabel(\"Update (Training)\", color=\"C0\")        \n",
    "    ax1.xaxis.grid(False)  \n",
    "    ax1.set_ylim((0,4))\n",
    "\n",
    "    ax2.plot(val_loss, color=\"C1\")\n",
    "    ax2.xaxis.tick_top()\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_xlabel('Epoch (Validation)', color=\"C1\")     \n",
    "    ax2.xaxis.set_label_position('top')     \n",
    "    ax2.xaxis.grid(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "    ax2.set_ylim((0,4))\n",
    "\n",
    "    ax4.plot(train_entropy, color=\"C3\")    \n",
    "    ax4.set_xlabel('Update (Training)', color=\"black\")     \n",
    "    ax4.set_ylabel(\"Entropy\", color=\"C3\")\n",
    "    ax4.tick_params(axis='x', colors=\"black\")\n",
    "    ax4.tick_params(axis='y', colors=\"black\")\n",
    "    ax4.xaxis.grid(False)\n",
    "\n",
    "    ax3.plot(val_acc, color=\"C2\")\n",
    "    ax3.set_xlabel(\"Epoch (Validation)\", color=\"black\")\n",
    "    ax3.set_ylabel(\"Accuracy\", color=\"C2\")\n",
    "    ax3.tick_params(axis='x', colors=\"black\")\n",
    "    ax3.tick_params(axis='y', colors=\"black\")\n",
    "    ax3.xaxis.grid(False)\n",
    "    ax3.set_ylim((0,1))\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "    \n",
    "                    \n",
    "def print_action(dataset, action):\n",
    "    action = action_mapping[action]\n",
    "    print(\"Left %.1f\" % action[0] if action[0] < 0 else \"Right %.1f\" %\n",
    "          action[0] if action[0] > 0 else \"Straight\")\n",
    "    print(\"Throttle %.1f\" % action[1])\n",
    "    print(\"Break %.1f\" % action[2])\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "\"\"\"\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "# Convert RBG image to grayscale and normalize by data statistics\n",
    "def rgb2gray(rgb, norm=True):    \n",
    "    # rgb image -> gray [0, 1]\n",
    "    gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "    if norm:\n",
    "        # normalize\n",
    "        gray = gray / 128. - 1.\n",
    "    return gray\n",
    "\n",
    "# Hide the score and interface part of the game\n",
    "def hide_hud(img):\n",
    "    img[84:] = 0\n",
    "    return img\n",
    "\n",
    "# Save your model in ONNX format for evaluation\n",
    "def save_as_onnx(torch_model, sample_input, model_path):\n",
    "    torch.onnx.export(torch_model,             # model being run\n",
    "                    sample_input,              # model input (or a tuple for multiple inputs)\n",
    "                    f=model_path,              # where to save the model (can be a file or file-like object)\n",
    "                    export_params=True,        # store the trained parameter weights inside the model file\n",
    "                    opset_version=13,          # the ONNX version to export the model to - see https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
    "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3QTC6sgj2Eq"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_m8Pf1ImRiv"
   },
   "outputs": [],
   "source": [
    "# Action space (map from continuous actions for steering, throttle and break to 25 action combinations)\n",
    "action_mapping = [\n",
    "    (0, 0, 0),  # no action\n",
    "    (0, 0.5, 0),  # half throttle\n",
    "    (0, 1, 0),  # full trottle\n",
    "    (0, 0, 0.5),  # half break\n",
    "    (0, 0, 1),  # full break\n",
    "    # steering left with throttle/break control\n",
    "    (-0.5, 0, 0),  # half left\n",
    "    (-1, 0, 0),  # full left\n",
    "    (-0.5, 0.5, 0),  # half left\n",
    "    (-1, 0.5, 0),  # full left\n",
    "    (-0.5, 1, 0),  # half left\n",
    "    (-1, 1, 0),  # full left\n",
    "    (-0.5, 0, 0.5),  # half left\n",
    "    (-1, 0, 0.5),  # full left\n",
    "    (-0.5, 0, 1),  # half left\n",
    "    (-1, 0, 1),  # full left\n",
    "    # steering right with throttle/break control\n",
    "    (0.5, 0, 0),  # half right\n",
    "    (1, 0, 0),  # full right\n",
    "    (0.5, 0.5, 0),  # half right\n",
    "    (1, 0.5, 0),  # full right\n",
    "    (0.5, 1, 0),  # half right\n",
    "    (1, 1, 0),  # full right\n",
    "    (0.5, 0, 0.5),  # half right\n",
    "    (1, 0, 0.5),  # full right\n",
    "    (0.5, 0, 1),  # half right\n",
    "    (1, 0, 1)  # full right\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqzqN11_e81k"
   },
   "source": [
    "### Partial Demonstration Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN5p4yUHk5oi"
   },
   "outputs": [],
   "source": [
    "# data path\n",
    "train_root = 'train'\n",
    "val_root = 'val'\n",
    "\n",
    "if not os.path.exists(train_root):\n",
    "    with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "        os.makedirs(train_root, exist_ok=True)\n",
    "        zip_ref.extractall(train_root)\n",
    "        \n",
    "if not os.path.exists(val_root):\n",
    "    with zipfile.ZipFile('val.zip', 'r') as zip_ref:\n",
    "        os.makedirs(val_root, exist_ok=True)\n",
    "        zip_ref.extractall(val_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujwQCYlaefRf"
   },
   "outputs": [],
   "source": [
    "# create transition object for partial demonstrations\n",
    "Transition = namedtuple('Transition', ['frames', 'actions'])\n",
    "\n",
    "# Since the demonstrations are partial files assuming that the collected data is too\n",
    "# large to fit into memory at once the Demonstration class utilizes an object \n",
    "# from the ParialDataset class to load and unload files from the file system.\n",
    "# This is a typical use case for very large datasets and should give you an idea \n",
    "# how to handle such issues.  \n",
    "class Demonstration(object):\n",
    "    def __init__(self, root_path):\n",
    "        assert (os.path.exists(root_path))\n",
    "        self.root_path = root_path\n",
    "        # assign list of data files found in the data root directory\n",
    "        self.data_files = sorted(os.listdir(root_path))\n",
    "        self.buffer = None\n",
    "\n",
    "    def __len__(self):\n",
    "        # this count returns the number of files in the data root folder\n",
    "        # and also if an online buffer is used or not\n",
    "        if self.buffer is not None:\n",
    "            return len(self.data_files) + 1\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def load(self, idx):\n",
    "        # check if online buffer is used and index is equals to len(data_files)\n",
    "        # load the online buffer\n",
    "        if self.buffer is not None and idx == len(self.data_files):\n",
    "            data = self.buffer\n",
    "            frames = data.frames\n",
    "            actions = data.actions\n",
    "        # otherwise load the indexed file\n",
    "        else:\n",
    "            # select an index at random from all files\n",
    "            file_name = self.data_files[idx]\n",
    "            file_path = os.path.join(self.root_path, file_name)\n",
    "            # load the selected file\n",
    "            data = np.load(file_path)\n",
    "            # get the respective properties from the files\n",
    "            frames = data[\"frames\"]\n",
    "            actions = data[\"actions\"]\n",
    "            # clean the memory from the data file\n",
    "            del data\n",
    "        # return the transitions\n",
    "        return Transition(frames=frames, actions=actions)\n",
    "\n",
    "    def append(self, frame, action):\n",
    "        # create online buffer to append new transitions\n",
    "        if self.buffer is None:\n",
    "            trans = Transition(frames=frame, actions=action)\n",
    "            self.buffer = PartialDemonstrationDataset(trans)\n",
    "        else:\n",
    "            self.buffer.append(frame, action)\n",
    "\n",
    "class PartialDemonstrationDataset(Dataset):\n",
    "    def __init__(self, data, img_stack=1, show_hud=True):\n",
    "        self.frames = data.frames\n",
    "        self.actions = data.actions\n",
    "        self.img_stack = img_stack\n",
    "        \n",
    "        if show_hud:\n",
    "            self.transforms = Compose([rgb2gray])\n",
    "        else:\n",
    "            self.transforms = Compose([hide_hud, rgb2gray])\n",
    "        self.action_mapping = {i: x for i, x in enumerate(action_mapping)}             \n",
    "        self.act_to_idx = {x: i for i, x in enumerate(action_mapping)}             \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.frames.shape[0] - self.img_stack\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, action = self.frames[idx:idx + self.img_stack], self.actions[\n",
    "            idx + self.img_stack - 1]\n",
    "        transformed = []\n",
    "        for i in range(len(frames)):\n",
    "            transformed.append(self.transforms(frames[i]))\n",
    "        transformed = np.stack(transformed, axis=0)\n",
    "        return transformed, self.act_to_idx[tuple(action)]\n",
    "\n",
    "    def append(self, frame, action):\n",
    "        self.frames = np.append(self.frames, frame, axis=0)\n",
    "        self.actions = np.append(self.actions, action, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vViEbSRZj_4x"
   },
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4HfBVVlhRwz"
   },
   "outputs": [],
   "source": [
    "# Partial Demonstration\n",
    "_data = np.load(os.path.join(train_root, 'train_0.npz'))\n",
    "_trans = Transition(frames=_data['frames'], actions=_data['actions'])\n",
    "dataset = PartialDemonstrationDataset(_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYljPrjekEEL"
   },
   "outputs": [],
   "source": [
    "# Action Statistics\n",
    "print(\"Number of samples: {}\".format(len(dataset)))\n",
    "act_to_idx = {v: k for k, v in dataset.action_mapping.items()}\n",
    "plt.hist([act_to_idx[tuple(action)] for action in dataset.actions], bins=list(range(25)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1itQJFxwkHYT"
   },
   "outputs": [],
   "source": [
    "idx = 1234\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(dataset[idx][0][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87zShnVElwv7"
   },
   "outputs": [],
   "source": [
    "# release memory\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcUOi1VRmPTm"
   },
   "source": [
    "# Define Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbVFFMoSmSpO"
   },
   "outputs": [],
   "source": [
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self, n_units_out):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        ########################\n",
    "        #### YOUR CODE HERE ####\n",
    "        ########################\n",
    "        # Note: the input to the network is one grayscale\n",
    "        # The dimension of the frames is 96x96\n",
    "        # Hence, the input tensor has shape [1, 96, 96]\n",
    "        \n",
    "        # Note 2: don't apply an activation function to the output layer\n",
    "        # Our loss function implicitly applies the softmax activation\n",
    "        # which is numerically more stable\n",
    "\n",
    "    def forward(self, x):\n",
    "        ########################\n",
    "        #### YOUR CODE HERE ####\n",
    "        ########################\n",
    "        # Process the batch with your defined network and\n",
    "        # return action predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNFcS2jmVL5"
   },
   "source": [
    "# Define Training and Validation Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo86GeoPhoMT"
   },
   "outputs": [],
   "source": [
    "def train(net, demos, loss_func, optimizer, logger, epoch, show_hud=False):\n",
    "    net.train()\n",
    "    # Initialize helpers variables\n",
    "    ts_len = len(demos)\n",
    "    running_loss = None\n",
    "    alpha = 0.3\n",
    "    # Iterate over the list of demonstration files\n",
    "    with tqdm(range(len(demos))) as pbar:\n",
    "        for i, idx in enumerate(BatchSampler(SubsetRandomSampler(range(ts_len)), 1, False)):\n",
    "            # Load the selected index from the filesystem\n",
    "            data = demos.load(idx[0])\n",
    "            # Create dataset from loaded data sub-set\n",
    "            partial = PartialDemonstrationDataset(data, show_hud=show_hud)\n",
    "            # Create dataloader\n",
    "            loader = DataLoader(partial, batch_size=batchsize, num_workers=1, shuffle=True, drop_last=False, pin_memory=True)\n",
    "            l_len = len(loader)\n",
    "            # Iterate over parial dataset\n",
    "            for j, (frame, action) in enumerate(loader):\n",
    "                frame = frame.float().to(device)\n",
    "                action = action.to(device)\n",
    "                # prediction\n",
    "                prediction = net(frame)                       \n",
    "                # loss\n",
    "                loss = loss_func(prediction, action)\n",
    "                # entropy\n",
    "                with torch.no_grad():\n",
    "                    probs = torch.softmax(prediction, dim=-1)\n",
    "                    entropy = torch.mean(-torch.sum(probs * torch.log(probs), dim=-1)) \n",
    "                # Update weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # log\n",
    "                logger.log(\"training_loss\", loss.item())\n",
    "                logger.log(\"training_entropy\", entropy.item())\n",
    "                # update progress\n",
    "                running_loss = loss.item() if running_loss is None else loss.item() * alpha + (1 - alpha) * running_loss\n",
    "                pbar.set_postfix_str(\"Epoch: %03d/%03d Partial: %03d/%03d Idx: %03d/%03d Loss: %.4f\" % (epoch, n_epochs, i+1, ts_len, j+1, l_len, running_loss))\n",
    "            pbar.update()\n",
    "    return frame  # serves as sample input for saving the model in ONNX format\n",
    "\n",
    "def val(net, demos, loss_func, logger, epoch, show_hud=False):\n",
    "    bs = batchsize\n",
    "    vs_len = len(demos)\n",
    "    net.eval()\n",
    "    loss_ = []\n",
    "    accuracies = []\n",
    "    # Iterate over the list of demonstration files\n",
    "    with tqdm(range(vs_len)) as pbar:\n",
    "        for i, idx in enumerate(BatchSampler(SubsetRandomSampler(range(vs_len)), 1, False)):\n",
    "            # Load the selected index from the filesystem\n",
    "            data = demos.load(idx[0])\n",
    "            # Create dataset from loaded data sub-set\n",
    "            partial = PartialDemonstrationDataset(data, show_hud=show_hud)\n",
    "            # Create dataloader\n",
    "            loader = DataLoader(partial, batch_size=batchsize, num_workers=1, shuffle=False, drop_last=False, pin_memory=True)\n",
    "            l_len = len(loader)\n",
    "            predictions = np.empty((len(partial,)), dtype=np.float32)\n",
    "            targets = np.empty((len(partial,)), dtype=np.float32)\n",
    "            # Iterate over parial dataset\n",
    "            for j, (frame, action) in enumerate(loader):\n",
    "                with torch.no_grad():\n",
    "                    frame = frame.float().to(device)\n",
    "                    action = action.to(device)\n",
    "                    # prediction\n",
    "                    prediction = net(frame)\n",
    "                    loss_.append(loss_func(prediction, action).cpu().item())\n",
    "                    # collect predictions and targets            \n",
    "                    prediction = torch.argmax(prediction.cpu(), dim=-1)\n",
    "                    predictions[j * bs:j * bs + len(prediction)] = prediction.cpu().numpy()\n",
    "                    targets[j * bs:j * bs + len(prediction)] = action.cpu().numpy()\n",
    "                    pbar.set_postfix_str(\"Validation Epoch: %03d/%03d Partial: %03d/%03d Idx: %03d/%03d\" % (epoch, n_epochs, i+1, vs_len, j+1, l_len))\n",
    "            acc = np.mean(targets == predictions)\n",
    "            accuracies.append(acc)\n",
    "            pbar.update()\n",
    "    # loss\n",
    "    accuracy = np.mean(accuracies)\n",
    "    # log\n",
    "    logger.log(\"validation_loss\", np.mean(loss_))\n",
    "    logger.log(\"validation_accuracy\", accuracy)\n",
    "    # --\n",
    "    return np.mean(loss_), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv8P8s4rnOZM"
   },
   "source": [
    "# Train your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsdxaB9FgkM7"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# choose your hyper-parameters\n",
    "\n",
    "learning_rate = ...\n",
    "weight_decay = ...\n",
    "batchsize = ...\n",
    "n_epochs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioJ4P7RhkX7o"
   },
   "outputs": [],
   "source": [
    "# Partial Datasets\n",
    "train_loader = Demonstration(train_root)\n",
    "val_loader = Demonstration(val_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM7ylckLnRnP"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))\n",
    "\n",
    "# Specify the google drive mount here if you want to store logs and weights there (and set it up earlier)\n",
    "logger = Logger(\"logdir\")\n",
    "print(\"Saving state to {}\".format(logger.basepath))\n",
    "\n",
    "# Network\n",
    "net = AgentNetwork(n_units_out=len(action_mapping))\n",
    "net = net.to(device)\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
    "\n",
    "# Loss\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training\n",
    "val_loss, val_acc = val(net, val_loader, loss_func, logger, 0)\n",
    "for i_ep in range(n_epochs):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
    "    print(\"Saving state to {}\".format(logger.basepath))\n",
    "    print(\"[%03d] Validation Loss: %.4f Accuracy: %.4f\" % (i_ep, val_loss, val_acc))\n",
    "    # plot current training state\n",
    "    if i_ep > 0:\n",
    "        plot_metrics(logger)\n",
    "    # train\n",
    "    sample_frame = train(net, train_loader, loss_func, optimizer, logger, i_ep + 1)\n",
    "    # validate\n",
    "    val_loss, val_acc = val(net, val_loader, loss_func, logger, i_ep + 1)\n",
    "    # LR schedule\n",
    "    scheduler.step()\n",
    "    # store logs\n",
    "    logger.dump()\n",
    "    # store weights\n",
    "    torch.save(net.state_dict(), logger.param_file)\n",
    "\n",
    "# Export agent as ONNX file\n",
    "save_as_onnx(net, sample_frame, logger.onnx_file)\n",
    "\n",
    "# --\n",
    "clear_output(wait=True)\n",
    "print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
    "print(\"Saved state to {}\".format(logger.basepath))\n",
    "print(\"[%03d] Validation Loss: %.4f Accuracy: %.4f\" % (i_ep + 1, val_loss, val_acc))\n",
    "plot_metrics(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PelO4qdwuwK"
   },
   "source": [
    "# Evaluate the agent in the real environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVrASW_Hy1lo"
   },
   "source": [
    "### Environment and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTFAzdfiaYmu"
   },
   "source": [
    "Here we create classes for our environment and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJeEIolVw0OQ"
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_stack=1, show_hud=True, record_video=True):\n",
    "        self.record_video=record_video\n",
    "        self.gym_env = gym.make('CarRacing-v0')\n",
    "        self.env = self.wrap_env(self.gym_env)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.img_stack = img_stack\n",
    "        self.show_hud = show_hud\n",
    "\n",
    "    def reset(self, raw_state=False):\n",
    "        self.env = self.wrap_env(self.gym_env)\n",
    "        self.rewards = []\n",
    "        img_rgb = self.env.reset()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        if not self.show_hud:\n",
    "            img_gray = hide_hud(img_gray)\n",
    "        self.stack = [img_gray] * self.img_stack\n",
    "        if raw_state:\n",
    "            return np.array(self.stack), np.array(img_rgb)\n",
    "        else:\n",
    "            return np.array(self.stack)\n",
    "\n",
    "    def step(self, action, raw_state=False):\n",
    "        img_rgb, reward, done, _ = self.env.step(action)            \n",
    "        # accumulate reward\n",
    "        self.rewards.append(reward)            \n",
    "        # if no reward recently, end the episode\n",
    "        die = True if np.mean(self.rewards[-np.minimum(100, len(self.rewards)):]) <= -1 else False\n",
    "        if done or die:\n",
    "            # print(done, die)\n",
    "            # print(self.gym_env.tile_visited_count, len(self.gym_env.track))\n",
    "            # print(self.rewards)\n",
    "            self.env.close()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        if not self.show_hud:\n",
    "            img_gray = hide_hud(img_gray)\n",
    "        # add to frame stack  \n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == self.img_stack\n",
    "        # --\n",
    "        if raw_state:\n",
    "            return np.array(self.stack), np.sum(self.rewards[-1]), done, die, img_rgb\n",
    "        else:\n",
    "            return np.array(self.stack), np.sum(self.rewards[-1]), done, die\n",
    "\n",
    "    def render(self, *arg):\n",
    "        return self.env.render(*arg)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def wrap_env(self, env):\n",
    "        if self.record_video:\n",
    "            env = wrap_env(env)\n",
    "        return env\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent for training\n",
    "    \"\"\"\n",
    "    def __init__(self, net, img_stack=1):\n",
    "        self.net = net\n",
    "        self.img_stack = img_stack\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.net(state)\n",
    "            if type(action_probs) in (tuple, list):\n",
    "                action_probs = action_probs[0]            \n",
    "        action, action_idx, a_logp = self.sample_action(action_probs)\n",
    "        a_logp = a_logp.item()\n",
    "\n",
    "        return action, action_idx, a_logp\n",
    "        \n",
    "    def sample_action(self, probs):\n",
    "        m = Categorical(logits=probs.to(\"cpu\"))\n",
    "        action_idx = m.sample()\n",
    "        a_logp = m.log_prob(action_idx)\n",
    "        action = action_mapping[int(action_idx.squeeze().cpu().numpy())]\n",
    "        return action, action_idx, a_logp\n",
    "    \n",
    "    def load_param(self, param_file):        \n",
    "        self.net.load_state_dict(torch.load(param_file))\n",
    "\n",
    "def run_episode(agent, show_progress=True, record_video=True, show_hud=False):\n",
    "    env = Env(img_stack=1, record_video=record_video, show_hud=show_hud)\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done_or_die = False\n",
    "    if show_progress:\n",
    "        progress = tqdm(desc=\"Score: 0\")\n",
    "    while not done_or_die:\n",
    "        action, action_idx, a_logp = agent.select_action(state)\n",
    "        state, reward, done, die = env.step(action)\n",
    "        score += reward\n",
    "        if show_progress:\n",
    "            progress.update()\n",
    "            progress.set_description(\"Score: {:.2f}\".format(score))\n",
    "        if done or die:\n",
    "            done_or_die = True\n",
    "    env.close()\n",
    "    if show_progress:\n",
    "        progress.close()    \n",
    "    if record_video:\n",
    "        show_video()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkjTFDi3y72N"
   },
   "source": [
    "## Evaluate behavioral cloning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0JaMOvxzBio"
   },
   "source": [
    "Let's see how the agent is doing in the real environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrqzgG-bzXws"
   },
   "outputs": [],
   "source": [
    "agent = Agent(net)\n",
    "agent.load_param(logger.param_file)\n",
    "print(logger.param_file)\n",
    "run_episode(agent, show_progress=True, record_video=True, show_hud=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtqNU8QXAaHP"
   },
   "source": [
    "Since we often have high variance when evaluating RL agents we should evaluate the agent multiple times to get a better feeling for its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WY5BeIKXAZs8"
   },
   "outputs": [],
   "source": [
    "n_eval_episodes = 10\n",
    "scores = []\n",
    "for i in tqdm(range(n_eval_episodes), desc=\"Episode\"):\n",
    "    scores.append(run_episode(agent, show_progress=False, record_video=False, show_hud=False))\n",
    "    print(\"Score: %d\" % scores[-1])\n",
    "print(\"Mean Score: %.2f (Std: %.2f)\" %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmDAORUGhJQD"
   },
   "source": [
    "# DAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Y8WBIkXvlP"
   },
   "source": [
    "Now we can implement DAgger, you have downloaded a relatively well trained model you can use as an expert for this purpose.\n",
    "\n",
    "Load expert model that is provided as ONNX file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqFaY_8ZprUw"
   },
   "source": [
    "## Load the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QxdNRHnpw4V"
   },
   "outputs": [],
   "source": [
    "# Load expert\n",
    "expert_net = ConvertModel(onnx.load(\"expert.onnx\"))\n",
    "expert_net = expert_net.to(device)\n",
    "# Freeze expert weights\n",
    "for p in expert_net.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmYXqfOZX0Yr"
   },
   "source": [
    "Next, you have to implement the DAgger algorithm (see slides for details). This function implements the core idea of DAgger:\n",
    "\n",
    "\n",
    "1. Choose the policy with probability beta\n",
    "2. Sample T-step trajectories using this policy\n",
    "3. Label the gathered states with the expert\n",
    "\n",
    "The aggregation and training part are already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jso18L4z2Gio"
   },
   "outputs": [],
   "source": [
    "# inner loop of DAgger\n",
    "def dagger(current_policy, expert_policy, beta=1., show_hud=False):\n",
    "    # Set up environment and result lists (here we show the HUD because the expert needs it)\n",
    "    env = Env(img_stack=1, record_video=False, show_hud=True)\n",
    "    state = env.reset()\n",
    "    # the expert agent was trained using the last 4 frames as input so we need to account for this\n",
    "    state_log = [state.squeeze()] * 4\n",
    "    frame_log = []\n",
    "    action_log = []\n",
    "\n",
    "    # Use this method to prepare the state for both policies\n",
    "    def prepare_state_for_policy(policy, state_log):\n",
    "        if show_hud or policy.img_stack == 4:\n",
    "            return np.array(state_log[-policy.img_stack:])\n",
    "        else:\n",
    "            return np.array([hide_hud(x) for x in state_log[-policy.img_stack:]])\n",
    "\n",
    "    #### YOUR CODE HERE ####\n",
    "    # Implement DAgger algorithm here\n",
    "    # 1) Choose a policy according to the DAgger algorithm (use beta)\n",
    "    # 2) Sample trajectory with this policy (here, we create one episode)\n",
    "    #     -> call \"policy.select_action(state)\" to predict the action for the current state\n",
    "    #     -> call \"prepare_state_for_policy(policy, state_log)\" to get the current state\n",
    "    #        in the correct format regardless of the chosen policy\n",
    "    # 3) Label the states of this trajectory with your expert\n",
    "    #     -> the expert policy always expects 4 frames, pass the state as \"np.array(state_log)\"\n",
    "    \n",
    "    #1: Choose policy\n",
    "    \n",
    "    #### YOUR CODE HERE ####    \n",
    "    \n",
    "    done_or_die = False\n",
    "    while not done_or_die:        \n",
    "        #2: Sample trajectory:\n",
    "        #   -> select action\n",
    "        #   -> perform action in the environment\n",
    "        #   -> pass \"raw_state=True\" to env.step() so you can record the \n",
    "        #      original frames without pre-processing (which we need to aggregate datasets later)\n",
    "        \n",
    "        #### YOUR CODE HERE ####\n",
    "                \n",
    "        #3: label the current state with the expert policy\n",
    "        \n",
    "        #### YOUR CODE HERE ####\n",
    "\n",
    "        # Always keep the last four frames in the log as we need them for the expert\n",
    "        state_log.pop(0)\n",
    "        state_log.append(state.squeeze())\n",
    "        \n",
    "        # Keep a record of states and actions so we can use them for training our agent\n",
    "        frame_log.append(raw_state)\n",
    "        action_log.append(expert_action)\n",
    "\n",
    "        # Check when you're done\n",
    "        if done or die:\n",
    "            done_or_die = True\n",
    "    env.close()    \n",
    "    return np.array(frame_log), np.array(action_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjzq072Pynyc"
   },
   "source": [
    "Now train the agent again using the DAgger algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXlZWzRsZ2dJ"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# choose your hyper-parameters\n",
    "beta = ...\n",
    "learning_rate = ...\n",
    "wight_decay = ...\n",
    "n_epochs = ...\n",
    "n_dagger_iterations = ...\n",
    "show_hud = False\n",
    "\n",
    "# Specify the google drive mount here if you want to store logs and weights there (and set it up earlier)\n",
    "logger = Logger(\"logdir_dagger\")\n",
    "print(\"Saving state to {}\".format(logger.basepath))\n",
    "\n",
    "# Re-load datasets (since we change the dataset during DAgger training)\n",
    "train_loader = Demonstration(train_root)\n",
    "val_loader = Demonstration(val_root)\n",
    "use_partial_demos = True\n",
    "\n",
    "# Your own policy network\n",
    "net = AgentNetwork(n_units_out=len(action_mapping))\n",
    "net = net.to(device)\n",
    "\n",
    "train_agent = Agent(net, img_stack=1)\n",
    "expert_agent = Agent(expert_net, img_stack=4)\n",
    "\n",
    "# Loss\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Training\n",
    "val_loss, val_acc = val(net, val_loader, loss_func, logger, 0, show_hud=show_hud)\n",
    "for i_ep in range(n_epochs):\n",
    "    clear_output(wait=True)    \n",
    "    print(\"Saving state to {}\".format(logger.basepath))\n",
    "    print(\"[%03d] Validation Loss: %.4f Accuracy: %.4f\" % (i_ep, val_loss, val_acc))\n",
    "    # create new samples using our expert    \n",
    "    for _ in tqdm(range(n_dagger_iterations), desc=\"Generating expert samples\"):\n",
    "        frames, actions = dagger(train_agent, expert_agent, beta=beta, show_hud=show_hud)\n",
    "        # Here we aggregate the datasets by appending the new samples\n",
    "        # to our training set\n",
    "        # act different based on partial or full dataloader\n",
    "        if use_partial_demos: train_loader.append(frames, actions)\n",
    "        else: train_set.append(frames, actions)\n",
    "\n",
    "    # plot current training state\n",
    "    if i_ep > 0:\n",
    "        plot_metrics(logger)\n",
    "\n",
    "    # train the agent on the aggregated dataset\n",
    "    sample_frame = train(net, train_loader, loss_func, optimizer, logger, i_ep + 1, show_hud=show_hud)\n",
    "\n",
    "    # validate\n",
    "    val_loss, val_acc = val(net, val_loader, loss_func, logger, i_ep + 1)    \n",
    "    \n",
    "    # store logs\n",
    "    logger.dump()\n",
    "    # store weights\n",
    "    torch.save(net.state_dict(), logger.param_file)\n",
    "\n",
    "# store the dagger agent\n",
    "save_as_onnx(net, sample_frame, logger.onnx_file)\n",
    "# --\n",
    "clear_output(wait=True)\n",
    "print(\"Saved state to {}\".format(logger.basepath))\n",
    "print(\"[%03d] Validation Loss: %.4f Accuracy: %.4f\" % (i_ep + 1, val_loss, val_acc))\n",
    "plot_metrics(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cck7HjamBNqq"
   },
   "source": [
    "## Evaluate DAgger Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF36IFw2BPK0"
   },
   "source": [
    "If you successfully implemented your agent and the DAgger algorithm you can now upload your submission.\n",
    "\n",
    "First, lets check how the agent performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iOeC98o9Pw-"
   },
   "outputs": [],
   "source": [
    "run_episode(train_agent, show_progress=True, record_video=True, show_hud=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktb1CI_49s4r"
   },
   "outputs": [],
   "source": [
    "n_eval_episodes = 10\n",
    "scores = []\n",
    "for i in tqdm(range(n_eval_episodes), desc=\"Episode\"):\n",
    "    scores.append(run_episode(train_agent, show_progress=False, record_video=False, show_hud=False))\n",
    "    print(\"Score: %d\" % scores[-1])\n",
    "print(\"Mean Score: %.2f (Std: %.2f)\" %(np.mean(scores), np.std(scores)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "03_ImitationLearning_Exercise_PartialLoader.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
